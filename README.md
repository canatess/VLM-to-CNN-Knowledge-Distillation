# CUB-200-2011 Knowledge Distillation Framework

A unified framework for **VLM (Vision-Language Model) to CNN knowledge distillation** for fine-grained bird classification using the CUB-200-2011 dataset.

This framework combines multiple knowledge distillation techniques in a single, well-structured codebase:
- **Logit-based distillation**: Transfer soft probability distributions from teacher to student
- **Attention-based distillation**: Transfer visual attention maps from CLIP to CNN
- **Combined distillation**: Utilize both logit and attention transfer simultaneously

## ğŸ¯ Features

- **Unified Architecture**: Single codebase supporting multiple KD techniques
- **Flexible Configuration**: YAML-based configuration system for easy experimentation
- **Multiple Student Architectures**: ResNet, VGG, MobileNet, EfficientNet, and more
- **CLIP Teacher**: Uses pre-trained CLIP vision-language model as teacher
- **Comprehensive Evaluation**: Top-1, Top-5 accuracy, inference speed measurements
- **Experiment Management**: Run and compare multiple experiments systematically

## ğŸ“ Project Structure

```
VLM-to-CNN-Knowledge-Distillation/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ base.yaml              # Base configuration
â”‚   â”œâ”€â”€ logit_kd.yaml          # Logit distillation config
â”‚   â”œâ”€â”€ attention_kd.yaml      # Attention distillation config
â”‚   â””â”€â”€ combined_kd.yaml       # Combined distillation config
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ data/                  # Dataset and data loading
â”‚   â”‚   â”œâ”€â”€ dataset.py         # CUB-200-2011 dataset implementation
â”‚   â”‚   â””â”€â”€ transforms.py      # Image transformations
â”‚   â”œâ”€â”€ models/                # Model implementations
â”‚   â”‚   â”œâ”€â”€ teacher.py         # CLIP teacher model
â”‚   â”‚   â””â”€â”€ student.py         # CNN student models
â”‚   â”œâ”€â”€ distillation/          # Knowledge distillation
â”‚   â”‚   â”œâ”€â”€ losses.py          # Distillation loss functions
â”‚   â”‚   â”œâ”€â”€ attention.py       # Attention processing utilities
â”‚   â”‚   â””â”€â”€ distiller.py       # High-level KD orchestrator
â”‚   â”œâ”€â”€ training/              # Training and evaluation
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop
â”‚   â”‚   â””â”€â”€ evaluator.py       # Evaluation utilities
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â”œâ”€â”€ config.py          # Configuration management
â”‚       â”œâ”€â”€ helpers.py         # Helper functions
â”‚       â””â”€â”€ metrics.py         # Metrics tracking
â”œâ”€â”€ scripts/                   # Execution scripts
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation script
â”‚   â””â”€â”€ run_experiments.py    # Run multiple experiments
â””â”€â”€ requirements.txt          # Python dependencies
```

## ğŸš€ Installation

### 1. Clone the repository

```bash
cd VLM-to-CNN-Knowledge-Distillation
```

### 2. Create virtual environment (recommended)

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Download CUB-200-2011 dataset

The dataset should be placed in the project root with the following structure:
```
CUB_200_2011/
â”œâ”€â”€ images/
â”œâ”€â”€ images.txt
â”œâ”€â”€ image_class_labels.txt
â”œâ”€â”€ train_test_split.txt
â””â”€â”€ classes.txt
```

## ğŸ’» Usage

### Basic Training

Train a student model with default configuration:

```bash
python scripts/train.py
```

### Train with Specific Configuration

Use predefined configurations for different KD methods:

```bash
# Logit-based distillation
python scripts/train.py --config configs/logit_kd.yaml

# Attention-based distillation
python scripts/train.py --config configs/attention_kd.yaml

# Combined distillation (logit + attention)
python scripts/train.py --config configs/combined_kd.yaml
```

### Custom Training

Override specific parameters:

```bash
python scripts/train.py \
    --config configs/base.yaml \
    --student_architecture mobilenetv3_small \
    --num_epochs 30 \
    --batch_size 64 \
    --learning_rate 0.001
```

### Train from Scratch (No Distillation)

```bash
python scripts/train.py \
    --pretrained false \
    --distillation_type none \
    --num_epochs 100
```

### Evaluate Trained Model

```bash
python scripts/evaluate.py \
    --model_path outputs/experiment/best_model.pth \
    --config outputs/experiment/config.yaml \
    --save_predictions \
    --measure_speed
```

### Run Comprehensive Experiments

Compare multiple architectures and distillation methods:

```bash
python scripts/run_experiments.py \
    --architectures resnet18 mobilenetv3_small vgg16 \
    --distillation_types none logit attention combined \
    --num_epochs 30 \
    --output_dir ./experiments
```

## ğŸ“Š Supported Architectures

### Student Models (CNN)
- ResNet: `resnet18`, `resnet34`, `resnet50`
- VGG: `vgg16`, `vgg19`
- MobileNet: `mobilenetv3_small`, `mobilenetv3_large`
- EfficientNet: `efficientnet_b0`, `efficientnet_b1`
- DenseNet: `densenet121`

### Teacher Model (VLM)
- CLIP: `openai/clip-vit-base-patch32` (default)

## âš™ï¸ Configuration

Key configuration parameters in `configs/base.yaml`:

```yaml
# Data
data_root: "CUB_200_2011"
batch_size: 32
image_size: 224

# Models
student_architecture: "resnet18"
teacher_model: "openai/clip-vit-base-patch32"

# Training
num_epochs: 50
learning_rate: 0.0003
optimizer: "adamw"

# Distillation
distillation_type: "combined"  # none, logit, attention, combined
alpha_ce: 1.0          # Weight for cross-entropy loss
alpha_kd: 1.0          # Weight for logit distillation
alpha_attention: 0.1   # Weight for attention distillation
temperature: 4.0       # Temperature for softmax
```

## ğŸ“ˆ Distillation Methods

### 1. Logit-Based Distillation
Transfers soft probability distributions from teacher to student using KL divergence.

```yaml
distillation_type: "logit"
alpha_kd: 1.0
temperature: 4.0
```

### 2. Attention-Based Distillation
Transfers visual attention maps from CLIP vision encoder to CNN feature maps.

```yaml
distillation_type: "attention"
alpha_attention: 0.5
attention_loss_type: "mse"  # mse, l1, or kl
```

### 3. Combined Distillation
Uses both logit and attention transfer simultaneously for maximum knowledge transfer.

```yaml
distillation_type: "combined"
alpha_ce: 1.0
alpha_kd: 1.0
alpha_attention: 0.1
```

## ğŸ“ Output Structure

Each experiment creates a directory with:

```
outputs/experiment_name/
â”œâ”€â”€ config.yaml              # Saved configuration
â”œâ”€â”€ best_model.pth          # Best model checkpoint
â”œâ”€â”€ history.json            # Training history
â”œâ”€â”€ evaluation_results.json # Evaluation metrics
â””â”€â”€ predictions.npz         # Model predictions (optional)
```

## ğŸ”¬ Example Workflow

```bash
# 1. Train a ResNet-18 with combined distillation
python scripts/train.py \
    --config configs/combined_kd.yaml \
    --student_architecture resnet18 \
    --experiment_name resnet18_combined

# 2. Evaluate the trained model
python scripts/evaluate.py \
    --model_path outputs/resnet18_combined/best_model.pth \
    --config outputs/resnet18_combined/config.yaml \
    --measure_speed

# 3. Compare with baseline (no distillation)
python scripts/train.py \
    --student_architecture resnet18 \
    --distillation_type none \
    --experiment_name resnet18_baseline
```

## ğŸ“ Citation

If you use this framework in your research, please cite:

```bibtex
@misc{VLM-to-CNN-Knowledge-Distillation,
  title={Unified Knowledge Distillation Framework for Fine-Grained Bird Classification},
  author={Can Ali AteÅŸ, Abdullah Enes ErgÃ¼n, Emre Ã‡oban},
  year={2025}
}
```

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- CUB-200-2011 dataset: [Caltech-UCSD Birds-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)
- CLIP model: [OpenAI CLIP](https://github.com/openai/CLIP)
- timm library: [PyTorch Image Models](https://github.com/huggingface/pytorch-image-models)

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub or contact [your-email@example.com].
