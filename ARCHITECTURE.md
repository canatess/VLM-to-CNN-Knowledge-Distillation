# Architecture Overview

## ðŸ—ï¸ System Architecture

This document provides a detailed overview of the unified knowledge distillation framework architecture.

## ðŸ“ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Interface                          â”‚
â”‚  (Config Files, Command Line, Python API)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Main Scripts Layer                         â”‚
â”‚  â€¢ train.py         â€¢ evaluate.py    â€¢ run_experiments.py  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Core Framework                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Data       â”‚   Models     â”‚ Distillation â”‚           â”‚
â”‚  â”‚              â”‚              â”‚              â”‚           â”‚
â”‚  â”‚  â€¢ Dataset   â”‚  â€¢ Teacher   â”‚  â€¢ Losses    â”‚           â”‚
â”‚  â”‚  â€¢ Loaders   â”‚  â€¢ Student   â”‚  â€¢ Attention â”‚           â”‚
â”‚  â”‚  â€¢ Transform â”‚  â€¢ Factory   â”‚  â€¢ Distiller â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚              â”‚              â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚           Training & Evaluation             â”‚           â”‚
â”‚  â”‚  â€¢ Trainer  â€¢ Evaluator  â€¢ Metrics          â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                        â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚              Utilities                       â”‚           â”‚
â”‚  â”‚  â€¢ Config  â€¢ Helpers  â€¢ Metrics  â€¢ Logging  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ” Component Details

### 1. Data Module (`src/data/`)

**Purpose**: Handle all data loading and preprocessing

**Components**:
- `dataset.py`: CUB-200-2011 dataset implementation
- `transforms.py`: Image augmentation and preprocessing

**Key Features**:
- Lazy loading for memory efficiency
- Stratified train/val split
- Support for both training and evaluation transforms
- Configurable batch size and workers

**Example Flow**:
```
Raw Images â†’ Transform â†’ Dataset â†’ DataLoader â†’ Batches
```

### 2. Models Module (`src/models/`)

**Purpose**: Define teacher and student architectures

**Components**:
- `teacher.py`: CLIP vision-language model (frozen)
- `student.py`: CNN architectures (trainable)

**Teacher Model (CLIPTeacher)**:
```
Input Image â†’ CLIP Vision Encoder â†’ {
    â€¢ Logits (for classification)
    â€¢ Attention Maps (optional)
    â€¢ Image Embeddings
}
```

**Student Model (StudentCNN)**:
```
Input Image â†’ CNN Backbone â†’ {
    â€¢ Logits (classification)
    â€¢ Feature Maps (for attention)
}
```

**Supported Architectures**:
- ResNet family (18, 34, 50)
- VGG family (16, 19)
- MobileNet (v3 small/large)
- EfficientNet (B0, B1)
- DenseNet (121)

### 3. Distillation Module (`src/distillation/`)

**Purpose**: Implement knowledge transfer mechanisms

**Components**:
- `losses.py`: All distillation loss functions
- `attention.py`: Attention map processing utilities
- `distiller.py`: High-level KD orchestrator

**Loss Functions**:

1. **Cross-Entropy Loss**:
   ```python
   L_CE = CrossEntropy(student_logits, ground_truth)
   ```

2. **Logit Distillation Loss** (KL Divergence):
   ```python
   L_KD = KL(softmax(student/T), softmax(teacher/T)) Ã— TÂ²
   ```

3. **Attention Distillation Loss**:
   ```python
   L_Attn = MSE(student_attention, teacher_attention)
   # or L1, KL variants
   ```

4. **Combined Loss**:
   ```python
   L_total = Î±_ce Ã— L_CE + Î±_kd Ã— L_KD + Î±_attn Ã— L_Attn
   ```

**Attention Processing Pipeline**:
```
CLIP Attention Weights â†’ Rollout â†’ Normalize â†’ Resize
                                                    â†“
CNN Feature Maps â†’ Spatial Attention â†’ Normalize â†’ Match Resolution
                                                    â†“
                                    Compute Loss â† â†
```

### 4. Training Module (`src/training/`)

**Purpose**: Orchestrate training and evaluation

**Components**:
- `trainer.py`: Main training loop with KD support
- `evaluator.py`: Evaluation metrics and utilities

**Training Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Load Batch                          â”‚
â”‚     â†“                                   â”‚
â”‚  2. Teacher Forward (frozen)            â”‚
â”‚     â€¢ Get teacher logits               â”‚
â”‚     â€¢ Get teacher attention (optional) â”‚
â”‚     â†“                                   â”‚
â”‚  3. Student Forward (trainable)         â”‚
â”‚     â€¢ Get student logits               â”‚
â”‚     â€¢ Get student attention (optional) â”‚
â”‚     â†“                                   â”‚
â”‚  4. Compute Losses                      â”‚
â”‚     â€¢ Cross-entropy                    â”‚
â”‚     â€¢ Distillation (optional)          â”‚
â”‚     â€¢ Attention (optional)             â”‚
â”‚     â†“                                   â”‚
â”‚  5. Backward & Update                   â”‚
â”‚     â€¢ Compute gradients                â”‚
â”‚     â€¢ Update student weights           â”‚
â”‚     â†“                                   â”‚
â”‚  6. Log & Evaluate                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Utils Module (`src/utils/`)

**Purpose**: Provide common utilities

**Components**:
- `config.py`: Configuration management (YAML/JSON)
- `helpers.py`: Helper functions (seed, device, paths)
- `metrics.py`: Metric tracking and statistics

## ðŸ”„ Data Flow Diagrams

### Training with Combined Distillation

```
Input Batch (images, labels)
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                  â†“                  â†“
    Teacher            Student           Ground Truth
  (CLIP frozen)      (CNN train)
        â”‚                  â”‚                  â”‚
        â”œâ”€ Logits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
        â”‚                  â”‚                  â”‚
        â”œâ”€ Attention â”€â”€â”€â”€â”€â”€â”¤                  â”‚
        â”‚                  â”‚                  â”‚
        â†“                  â†“                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Loss Computation                   â”‚
    â”‚  â€¢ CE Loss (student vs labels)            â”‚
    â”‚  â€¢ KD Loss (student vs teacher logits)    â”‚
    â”‚  â€¢ Attn Loss (student vs teacher attn)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
              Weighted Sum â†’ Total Loss
                     â†“
              Backward Pass
                     â†“
         Update Student Weights
```

### Evaluation Flow

```
Test Batch
    â†“
Student Model (eval mode)
    â†“
Predictions
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compute Metrics   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Accuracy        â”‚
â”‚ â€¢ Top-5 Acc       â”‚
â”‚ â€¢ Loss            â”‚
â”‚ â€¢ Per-class Acc   â”‚
â”‚ â€¢ Confusion Mat   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Save Results
```

## ðŸŽ¯ Design Patterns

### 1. Factory Pattern
Used for model creation:
```python
def build_student(architecture: str, ...) -> StudentCNN:
    return StudentCNN(architecture=architecture, ...)
```

### 2. Strategy Pattern
Different distillation strategies:
```python
distiller = KnowledgeDistiller(
    distillation_type="logit"  # or "attention", "combined"
)
```

### 3. Builder Pattern
Configuration building:
```python
config = Config()
config.update(yaml_config)
config.update(cli_args)
```

### 4. Observer Pattern
Metrics tracking:
```python
tracker = MetricsTracker()
tracker.update({"loss": loss, "acc": acc})
```

## ðŸ“Š Configuration System

### Configuration Hierarchy

```
base.yaml (defaults)
    â†“
specific_config.yaml (overrides)
    â†“
command line args (final overrides)
    â†“
Runtime Config Object
```

### Config Loading Flow

```python
# 1. Load base config
config = Config()  # Default values

# 2. Load from file (optional)
if config_file:
    config = Config.load(config_file)

# 3. Override with CLI args
for arg, value in cli_args:
    setattr(config, arg, value)

# 4. Validate and use
config.validate()
```

## ðŸ”§ Extension Points

### Adding New Student Architecture

1. Check if supported by timm:
   ```python
   timm.list_models("*your_arch*")
   ```

2. Add to ARCH_MAPPING in `student.py` (if needed)

3. Use in config:
   ```yaml
   student_architecture: "your_new_arch"
   ```

### Adding New Loss Function

1. Implement in `distillation/losses.py`:
   ```python
   def my_custom_loss(student_output, teacher_output) -> Tensor:
       # Your implementation
       return loss
   ```

2. Register in `combined_loss()` function

3. Add weight parameter to Config

### Adding New KD Method

1. Create new file in `distillation/`
2. Implement loss computation
3. Update `KnowledgeDistiller` class
4. Add configuration option

## ðŸ“ˆ Performance Considerations

### Memory Optimization
- Teacher model frozen (no gradients stored)
- Optional gradient checkpointing for large models
- Efficient data loading with pinned memory
- Optional mixed precision (AMP)

### Speed Optimization
- Multi-worker data loading
- Non-blocking GPU transfers
- Compiled models (torch.compile in future)
- Batch processing

### Scalability
- Modular design allows distributed training
- Configuration system supports hyperparameter search
- Efficient evaluation with minimal overhead

## ðŸ§ª Testing Strategy

### Unit Tests (Planned)
- Data loading correctness
- Model output shapes
- Loss computation
- Metric calculation

### Integration Tests (Planned)
- End-to-end training pipeline
- Configuration loading
- Model saving/loading
- Evaluation accuracy

### Smoke Tests
- Quick training run (1 epoch)
- All architectures loadable
- All configs valid

## ðŸ“š Dependencies

### Core Dependencies
```
torch >= 2.0.0          # PyTorch framework
torchvision >= 0.15.0   # Vision utilities
transformers >= 4.30.0  # CLIP model
timm >= 0.9.0           # CNN architectures
```

### Why These Versions?
- **PyTorch 2.0+**: Native AMP, better performance
- **Transformers 4.30+**: Stable CLIP implementation
- **timm 0.9+**: Wide architecture support, pretrained weights

## ðŸŽ“ Best Practices

### Code Organization
âœ… Clear separation of concerns
âœ… Type hints for better IDE support
âœ… Comprehensive docstrings
âœ… Modular, reusable components

### Configuration Management
âœ… YAML for human readability
âœ… Defaults in code, overrides in config
âœ… Validation before training
âœ… Save config with results

### Experiment Tracking
âœ… Unique experiment names
âœ… Timestamped output directories
âœ… Save full configuration
âœ… Log all metrics

### Performance
âœ… Profile before optimizing
âœ… Use appropriate batch sizes
âœ… Enable mixed precision when possible
âœ… Monitor GPU utilization

This architecture provides a solid foundation for research in knowledge distillation while remaining flexible and extensible for future improvements!
